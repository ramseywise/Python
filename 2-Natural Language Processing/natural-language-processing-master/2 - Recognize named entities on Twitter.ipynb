{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.txt is already downloaded.\n",
      "File data/validation.txt is already downloaded.\n",
      "File data/test.txt is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week2_resources\n",
    "\n",
    "download_week2_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wiseer85/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import sklearn_crfsuite\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Data \n",
    "\n",
    "We will work with the Twitter named entitty recognition corpus, which contains twits with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6519"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tags and tokens for reading data\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "            if token.startswith(\"http://\") or token.startswith(\"https://\"): token = \"<URL>\"\n",
    "            elif token.startswith(\"@\"): token = \"<USR>\"\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag) \n",
    "    return tokens, tags\n",
    "\n",
    "# Read data for training, optimizing and evaluating model\n",
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')\n",
    "\n",
    "# Check count of train and validation tokens\n",
    "len(train_tokens+validation_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build token dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index tokens and tags\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    for i,token_tag in enumerate( special_tokens ):\n",
    "        tok2idx[token_tag] = i \n",
    "        idx2tok.append( token_tag )\n",
    "        \n",
    "    tokens_or_tags = [ item for sentence in tokens_or_tags for item in sentence ]\n",
    "    \n",
    "    length_special = len( special_tokens ) \n",
    "    \n",
    "    i=0\n",
    "    for token_tag in (tokens_or_tags):\n",
    "        if token_tag not in tok2idx.keys():\n",
    "            tok2idx[token_tag] = i + length_special\n",
    "            idx2tok.append( token_tag )\n",
    "            i += 1\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20505\n",
      "20505\n",
      "21\n",
      "21\n",
      "79\n",
      "me\n"
     ]
    }
   ],
   "source": [
    "#<UNK> token for out of vocabulary tokens;\n",
    "#<PAD> token for padding sentence to the same length when we create batches of sentences.\n",
    "\n",
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
    "\n",
    "print(len(idx2token))\n",
    "print(len(token2idx))\n",
    "print(len(idx2tag))\n",
    "print(len(tag2idx))\n",
    "print(token2idx['me'])\n",
    "print(idx2token[79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping functions between tokens and ids\n",
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate batches\n",
    "\n",
    "Neural Networks are trained with batches so that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates padded batches of tokens and tags\n",
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a recurrent neural network\n",
    "\n",
    "We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we need to create [placeholders](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder) to specify what data we are going to feed into the network during the execution time.  For this task we will need the following placeholders:\n",
    " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
    " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
    "\n",
    "It could be noticed that we use *None* in the shapes in the declaration, which means that data of any size can be feeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholder function\n",
    "def declare_placeholders(self):\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    # Placeholder for a dropout keep probability (default=1.0).\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[],name='dropout_ph')\n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder( dtype=tf.float32, shape=[],name= 'learning_rate_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to perform some preparatory steps: \n",
    " \n",
    "- Create embeddings matrix with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable). Specify its name (*embeddings_matrix*), type  (*tf.float32*), and initialize with random values.\n",
    "- Create forward and backward LSTM cells. TensorFlow provides a number of [RNN cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods) ready for you. We suggest that you use *BasicLSTMCell*, but you can also experiment with other types, e.g. GRU cells. [This](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blogpost could be interesting if you want to learn more about the differences.\n",
    "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
    " \n",
    "After that, you can build the computation graph that transforms an input_batch:\n",
    "\n",
    "- [Look up](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) embeddings for an *input_batch* in the prepared *embedding_matrix*.\n",
    "- Pass the embeddings through [Bidirectional Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) with the specified forward and backward cells. Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
    "- Create a dense layer on top. Its output will be used directly in loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify bi-LSTM architecture and computes logits for inputs\n",
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_value= initial_embedding_matrix, dtype=tf.float32, name='initial_embedding_matrix' )   \n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell( num_units= n_hidden_rnn ), output_keep_prob=self.dropout_ph ) \n",
    "    backward_cell =tf.contrib.rnn.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell( num_units= n_hidden_rnn ), output_keep_prob=self.dropout_ph )\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup( params= embedding_matrix_variable, ids= self.input_batch )  \n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(dtype=tf.float32 ,cell_fw= forward_cell , cell_bw= backward_cell,inputs= embeddings, sequence_length = self.lengths )######### YOUR CODE HERE #############\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the actual predictions of the neural network, you need to apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms logits to probabilities (softmax) and finds the most probable tags (argmax)\n",
    "def compute_predictions(self):\n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    softmax_output = tf.nn.softmax( self.logits )\n",
    "    # Use argmax (tf.argmax) to get the most probable tags (axis=-1)\n",
    "    self.predictions = tf.argmax( input= softmax_output,axis=-1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "During training we do not need predictions of the network, but we need a loss function. We will use [cross-entropy loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), efficiently implemented in TF as \n",
    "[cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits). Note that it should be applied to logits of the model (not to softmax probabilities!). Also note,  that we do not want to take into account loss terms coming from `<PAD>` tokens. So we need to mask them out, before computing [mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that computes masked cross-entopy loss with logits\n",
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits( logits= self.logits  ,labels=ground_truth_tags_one_hot )     \n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    self.loss = tf.reduce_mean(input_tensor= loss_tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to specify is how we want to optimize the loss. \n",
    "We suggest that you use [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) optimizer with a learning rate from the corresponding placeholder. \n",
    "You will also need to apply [clipping](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_clipping) to eliminate exploding gradients. It can be easily done with [clip_by_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and train_op for the model\n",
    "def perform_optimization(self):\n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer =  tf.train.AdamOptimizer(learning_rate= self.learning_rate_ph)  \n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm( grid , clip_norm=clip_norm ),var) for grid,var in self.grads_and_vars]  \n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Bi-LSTM class\n",
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the network and predict tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*, which was declared in *perform_optimization*. To predict tags, we just need to compute *self.predictions*. Anyway, we need to feed actual data through the placeholders that we defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train on batch function\n",
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *predict_for_batch* by initializing *feed_dict* with input *x_batch* and *lengths* and running the *session* for *self.predictions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict for batch function\n",
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.lengths: lengths}\n",
    "    predictions = session.run( self.predictions, feed_dict=feed_dict )    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation \n",
    "To simplify the evaluation process we provide two functions for you:\n",
    " - *predict_tags*: uses a model to get predictions and transforms indices to tokens and tags;\n",
    " - *eval_conll*: calculates precision, recall and F1 for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform predictions and transform indices to tokens and tags\n",
    "def predict_tags(model, session, token_idxs_batch, lengths):    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "# Define function to compute NER quality measures using CONLL shared task script\n",
    "def eval_conll(model, session, tokens, tags, short_report=True):    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # Extend prediction and ground truth sequence with 'O' tag to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create *BiLSTMModel* model with the following parameters:\n",
    " - *vocabulary_size* — number of tokens;\n",
    " - *n_tags* — number of tags;\n",
    " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
    " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
    " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
    "\n",
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- 4 epochs;\n",
    "- starting value of *learning_rate*: 0.005\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-7c28198a5783>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build BiLSTIMModel \n",
    "token2idx['<PAD>']\n",
    "len(idx2token)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(idx2token), n_tags=len(idx2tag), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'] )  \n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 4 \n",
    "learning_rate = 0.005 \n",
    "learning_rate_decay = np.sqrt(2)\n",
    "dropout_keep_probability = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 74518 phrases; correct: 199.\n",
      "\n",
      "precision:  0.27%; recall:  4.43%; F1:  0.50\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 8964 phrases; correct: 21.\n",
      "\n",
      "precision:  0.23%; recall:  3.91%; F1:  0.44\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 1685 phrases; correct: 715.\n",
      "\n",
      "precision:  42.43%; recall:  15.93%; F1:  23.16\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 143 phrases; correct: 67.\n",
      "\n",
      "precision:  46.85%; recall:  12.48%; F1:  19.71\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4863 phrases; correct: 2702.\n",
      "\n",
      "precision:  55.56%; recall:  60.19%; F1:  57.78\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 372 phrases; correct: 164.\n",
      "\n",
      "precision:  44.09%; recall:  30.54%; F1:  36.08\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4728 phrases; correct: 3842.\n",
      "\n",
      "precision:  81.26%; recall:  85.59%; F1:  83.37\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 402 phrases; correct: 181.\n",
      "\n",
      "precision:  45.02%; recall:  33.71%; F1:  38.55\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4574 phrases; correct: 4190.\n",
      "\n",
      "precision:  91.60%; recall:  93.34%; F1:  92.46\n",
      "\n",
      "\t     company: precision:   94.51%; recall:   93.78%; F1:   94.15; predicted:   638\n",
      "\n",
      "\t    facility: precision:   86.63%; recall:   94.90%; F1:   90.58; predicted:   344\n",
      "\n",
      "\t     geo-loc: precision:   94.92%; recall:   97.49%; F1:   96.19; predicted:  1023\n",
      "\n",
      "\t       movie: precision:   67.57%; recall:   73.53%; F1:   70.42; predicted:    74\n",
      "\n",
      "\t musicartist: precision:   78.23%; recall:   91.38%; F1:   84.29; predicted:   271\n",
      "\n",
      "\t       other: precision:   93.03%; recall:   95.24%; F1:   94.13; predicted:   775\n",
      "\n",
      "\t      person: precision:   95.52%; recall:   96.16%; F1:   95.84; predicted:   892\n",
      "\n",
      "\t     product: precision:   88.41%; recall:   91.19%; F1:   89.78; predicted:   328\n",
      "\n",
      "\t  sportsteam: precision:   86.98%; recall:   76.96%; F1:   81.66; predicted:   192\n",
      "\n",
      "\t      tvshow: precision:   70.27%; recall:   44.83%; F1:   54.74; predicted:    37\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 375 phrases; correct: 194.\n",
      "\n",
      "precision:  51.73%; recall:  36.13%; F1:  42.54\n",
      "\n",
      "\t     company: precision:   66.67%; recall:   53.85%; F1:   59.57; predicted:    84\n",
      "\n",
      "\t    facility: precision:   44.83%; recall:   38.24%; F1:   41.27; predicted:    29\n",
      "\n",
      "\t     geo-loc: precision:   70.24%; recall:   52.21%; F1:   59.90; predicted:    84\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     5\n",
      "\n",
      "\t musicartist: precision:   33.33%; recall:   21.43%; F1:   26.09; predicted:    18\n",
      "\n",
      "\t       other: precision:   37.31%; recall:   30.86%; F1:   33.78; predicted:    67\n",
      "\n",
      "\t      person: precision:   54.90%; recall:   25.00%; F1:   34.36; predicted:    51\n",
      "\n",
      "\t     product: precision:   12.50%; recall:    8.82%; F1:   10.34; predicted:    24\n",
      "\n",
      "\t  sportsteam: precision:   36.36%; recall:   20.00%; F1:   25.81; predicted:    11\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 430 phrases; correct: 242.\n",
      "\n",
      "precision:  56.28%; recall:  40.07%; F1:  46.81\n",
      "\n",
      "\t     company: precision:   60.34%; recall:   41.67%; F1:   49.30; predicted:    58\n",
      "\n",
      "\t    facility: precision:   44.00%; recall:   46.81%; F1:   45.36; predicted:    50\n",
      "\n",
      "\t     geo-loc: precision:   80.36%; recall:   54.55%; F1:   64.98; predicted:   112\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "\t musicartist: precision:   15.79%; recall:   11.11%; F1:   13.04; predicted:    19\n",
      "\n",
      "\t       other: precision:   39.58%; recall:   36.89%; F1:   38.19; predicted:    96\n",
      "\n",
      "\t      person: precision:   66.67%; recall:   40.38%; F1:   50.30; predicted:    63\n",
      "\n",
      "\t     product: precision:   28.57%; recall:   14.29%; F1:   19.05; predicted:    14\n",
      "\n",
      "\t  sportsteam: precision:   57.14%; recall:   25.81%; F1:   35.56; predicted:    14\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print validation results\n",
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
