{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate with seq2seq model\n",
    "\n",
    "Use neural networks to solve sequence-to-sequence prediction tasks. Seq2Seq models are very popular these days because they achieve great results in Machine Translation, Text Summarization, Conversational Modeling and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates pairs of equations and solutions to them\n",
    "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
    "    sample = []\n",
    "    for _ in range(dataset_size):\n",
    "        equation = (str(random.randint(min_value,max_value))+\n",
    "                   allowed_operators[random.randint(0,len(allowed_operators)-1)]+\n",
    "                    str(random.randint(min_value,max_value))\n",
    "                   )\n",
    "        solution = str(eval(equation))\n",
    "        sample.append((equation,solution))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correctness of your implementation\n",
    "def test_generate_equations():\n",
    "    allowed_operators = ['+', '-']\n",
    "    dataset_size = 10\n",
    "    for (input_, output_) in generate_equations(allowed_operators, dataset_size, 0, 100):\n",
    "        if not (type(input_) is str and type(output_) is str):\n",
    "            return \"Both parts should be strings.\"\n",
    "        if eval(input_) != int(output_):\n",
    "            return \"The (equation: {!r}, solution: {!r}) pair is incorrect.\".format(input_, output_)\n",
    "    return \"Tests passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_generate_equations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the train and test data for the neural network\n",
    "allowed_operators = ['+', '-']\n",
    "dataset_size = 100000\n",
    "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=9999)\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data for the neural network\n",
    "\n",
    "Generate the vocabulary by creating mappings of the characters to their indices in some vocabulary. Other strategies that your should consider are: data normalization (lowercasing, tokenization, how to consider punctuation marks), separate vocabulary for input and for output (e.g. for machine translation), some specifics of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for other tasks (filter frequent words if word2id is the basic unit of the sequence)\n",
    "word2id = {symbol:i for i, symbol in enumerate('^$#+-1234567890')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate special symbols \n",
    "start_symbol = '^' # beginning of decoding procedure \n",
    "end_symbol = '$' # end of a string, both input and output sequences\n",
    "padding_symbol = '#' #padding character to make lengths of all strings equal in training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentence to a list of vocabulary word indices \n",
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    sent_ids = [word2id[w] for w in sentence[:padded_len]] \n",
    "    if padded_len>len(sentence):\n",
    "        sent_ids.append(word2id['$'])\n",
    "        sent_ids +=[word2id['#']]*(padded_len-len(sent_ids))\n",
    "        return sent_ids,len(sentence)+1\n",
    "    sent_ids[-1]=word2id['$']\n",
    "    return sent_ids, padded_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check implementation\n",
    "def test_sentence_to_ids():\n",
    "    sentences = [(\"123+123\", 7), (\"123+123\", 8), (\"123+123\", 10)]\n",
    "    expected_output = [([5, 6, 7, 3, 5, 6, 1], 7), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1], 8), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1, 2, 2], 8)] \n",
    "    for (sentence, padded_len), (sentence_ids, expected_length) in zip(sentences, expected_output):\n",
    "        output, length = sentence_to_ids(sentence, word2id, padded_len)\n",
    "        if output != sentence_ids:\n",
    "            return(\"Convertion of '{}' for padded_len={} to {} is incorrect.\".format(\n",
    "                sentence, padded_len, output))\n",
    "        if length != expected_length:\n",
    "            return(\"Convertion of '{}' for padded_len={} has incorrect actual length {}.\".format(\n",
    "                sentence, padded_len, length))\n",
    "    return(\"Tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_to_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to return padded indices to symbols\n",
    "def ids_to_sentence(ids, id2word): \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches of indices\n",
    "def batch_to_ids(sentences, word2id, max_len):    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches with set size\n",
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('3764-3386', '378')\n",
      "Ids: [[7, 11, 10, 8, 4, 7, 7, 12, 10, 1], [7, 11, 12, 1, 2, 2, 2, 2, 2, 2]]\n",
      "Sentences lengths: [10, 4]\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "sentences = train_set[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)\n",
    "print('Input:', sentences)\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder-Decoder architecture\n",
    "\n",
    "Encoder-Decoder is a successful architecture for Seq2Seq tasks with different lengths of input and output sequences. The main idea is to use two recurrent neural networks, where the first neural network *encodes* the input sequence into a real-valued vector and then the second neural network *decodes* this vector into the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network architecture\n",
    "class Seq2SeqModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders to specify data feed\n",
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "    \n",
    "    # Placeholders for input and its actual lengths.\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "    \n",
    "    # Placeholders for groundtruth and its actual lengths.\n",
    "    self.ground_truth = tf.placeholder(shape=(None,None), dtype=tf.int32,name='ground_truth')\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None,), dtype=tf.int32,name='ground_truth_lengths')\n",
    "        \n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "\n",
    "Seq2SeqModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify embedding layers of neural network \n",
    "def create_embeddings(self, vocab_size, embeddings_size):\n",
    "    random_initializer = tf.random_uniform((vocab_size, embeddings_size), -1.0, 1.0)\n",
    "    self.embeddings =tf.Variable(initial_value=random_initializer,name='embeddings',dtype=tf.float32) \n",
    "    \n",
    "    # Perform embeddings lookup for self.input_batch. \n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings,self.input_batch)\n",
    "    \n",
    "Seq2SeqModel.__create_embeddings = classmethod(create_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The first RNN of the current architecture is called an *encoder* and serves for encoding an input sequence to a real-valued vector. Input of this RNN is an embedded input batch. Since sentences in the same batch could have different actual lengths, we also provide input lengths to avoid unnecessary computations. The final encoder state will be passed to the second RNN (decoder), which we will create soon. \n",
    "\n",
    "- TensorFlow provides a number of [RNN cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods) ready for use. We suggest that you use [GRU cell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GRUCell), but you can also experiment with other types. \n",
    "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify input keep probability using the dropout placeholder that we created before.\n",
    "- Combine the defined encoder cells with [Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). Use the embedded input batches and their lengths here.\n",
    "- Use *dtype=tf.float32* everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify RNN encoder architecture to compute input sequence to a real-valued vector\n",
    "def build_encoder(self, hidden_size):    \n",
    "    # Create GRUCell with dropout.\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(num_units=hidden_size)\n",
    "    encoder_cell = tf.contrib.rnn.DropoutWrapper(encoder_cell,input_keep_prob=self.dropout_ph,dtype=tf.float32)\n",
    "    # Create RNN with the predefined cell.\n",
    "    _, self.final_encoder_state = tf.nn.dynamic_rnn(encoder_cell, self.input_batch_embedded,\n",
    "        sequence_length = self.input_batch_lengths, dtype = tf.float32)\n",
    "    \n",
    "Seq2SeqModel.__build_encoder = classmethod(build_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify decoder architecture for computing output sequence\n",
    "# Two helpers are used: TrainingHelper and GreedyEmbeddingHelper\n",
    "def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "    # Use start symbols as the decoder inputs at the first time step.\n",
    "    batch_size = tf.shape(self.input_batch)[0]\n",
    "    start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "    ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "    \n",
    "    # Use the embedding layer defined before to lookup embedings for ground_truth_as_input. \n",
    "    self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings,ground_truth_as_input)\n",
    "     \n",
    "    # Create TrainingHelper for the train stage.\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded, \n",
    "                                                     self.ground_truth_lengths)\n",
    "    # Create GreedyEmbeddingHelper for the inference stage.\n",
    "    # You should provide the embedding layer, start_tokens and index of the end symbol.\n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings,start_tokens,end_symbol_id)    \n",
    "  \n",
    "    # Define decoder and return results\n",
    "    def decode(helper, scope, reuse=None):        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "            decoder_cell = tf.contrib.rnn.GRUCell(num_units=hidden_size,reuse=reuse)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell,input_keep_prob=self.dropout_ph)\n",
    "            \n",
    "            # Create a projection wrapper.\n",
    "            decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "            \n",
    "            # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "            # The initial state should be equal to the final state of the encoder!\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell,\n",
    "                helper=helper,\n",
    "                initial_state=self.final_encoder_state)\n",
    "            \n",
    "            \n",
    "            # The first returning argument of dynamic_decode contains two fields: 1) rnn_output (predicted logits) and 2) sample_id (predictions)\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=max_iter, \n",
    "                                                              output_time_major=False, impute_finished=True)\n",
    "            return outputs\n",
    "        \n",
    "    self.train_outputs = decode(train_helper, 'decode')\n",
    "    self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "    \n",
    "Seq2SeqModel.__build_decoder = classmethod(build_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sequence loss (masked cross-entopy loss with logits)\n",
    "def compute_loss(self):\n",
    "    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)    \n",
    "    self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        self.train_outputs.rnn_output,\n",
    "        self.ground_truth,\n",
    "        weights,\n",
    "        average_across_timesteps=True,\n",
    "        average_across_batch=True,\n",
    "        softmax_loss_function=None,\n",
    "        name=None\n",
    "    )\n",
    "    \n",
    "Seq2SeqModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify train_op to optimize self.loss\n",
    "def perform_optimization(self):\n",
    "    self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss = self.loss,\n",
    "            global_step = tf.train.get_global_step(),\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            optimizer='Adam',\n",
    "            clip_gradients=1.0\n",
    ")\n",
    "    \n",
    "Seq2SeqModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def init_model(self, vocab_size, embeddings_size, hidden_size, \n",
    "               max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "    \n",
    "    self.__declare_placeholders()\n",
    "    self.__create_embeddings(vocab_size, embeddings_size)\n",
    "    self.__build_encoder(hidden_size)\n",
    "    self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "    \n",
    "    # Compute loss and back-propagate.\n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()\n",
    "    \n",
    "    # Get predictions for evaluation.\n",
    "    self.train_predictions = self.train_outputs.sample_id\n",
    "    self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "Seq2SeqModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the network and predict output\n",
    "\n",
    "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*. To predict output, we just need to compute *self.infer_predictions*. In any case, we need to feed actual data through the placeholders that we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "    pred, loss, _ = session.run([\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "    return pred, loss\n",
    "\n",
    "Seq2SeqModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement function to predict output for some input sequence\n",
    "def predict_for_batch(self, session, X, X_seq_len):\n",
    "    feed_dict = {\n",
    "        self.input_batch: X,\n",
    "        self.input_batch_lengths: X_seq_len\n",
    "    }\n",
    "    \n",
    "    pred = session.run([\n",
    "            self.infer_predictions\n",
    "        ], feed_dict=feed_dict)[0]\n",
    "    return pred\n",
    "\n",
    "Seq2SeqModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute loss and validate results\n",
    "def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len\n",
    "        }    \n",
    "    \n",
    "    pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "    return pred, loss\n",
    "\n",
    "Seq2SeqModel.predict_for_batch_with_loss = classmethod(predict_for_batch_with_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run experiment and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    vocab_size = len(word2id),\n",
    "    embeddings_size=20,\n",
    "    max_iter=7,\n",
    "    hidden_size=512,\n",
    "    start_symbol_id=word2id['^'],\n",
    "    end_symbol_id=word2id['$'],\n",
    "    padding_symbol_id = word2id['#']\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.5\n",
    "max_len = 20\n",
    "\n",
    "n_step = int(len(train_set) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/10], step: [1/625], loss: 2.709073\n",
      "Epoch: [1/10], step: [201/625], loss: 1.821524\n",
      "Epoch: [1/10], step: [401/625], loss: 1.693949\n",
      "Epoch: [1/10], step: [601/625], loss: 1.664004\n",
      "Test: epoch 1 loss: 1.5824859\n",
      "X: 2444-171$#\n",
      "Y: 2273$#\n",
      "O: 2991$^\n",
      "\n",
      "X: 7054-4337$\n",
      "Y: 2717$#\n",
      "O: 2999$^\n",
      "\n",
      "X: 7958-1360$\n",
      "Y: 6598$#\n",
      "O: 7477$^\n",
      "\n",
      "0\n",
      "Train: epoch 2\n",
      "Epoch: [2/10], step: [1/625], loss: 1.601699\n",
      "Epoch: [2/10], step: [201/625], loss: 1.566108\n",
      "Epoch: [2/10], step: [401/625], loss: 1.512849\n",
      "Epoch: [2/10], step: [601/625], loss: 1.517137\n",
      "Test: epoch 2 loss: 1.414193\n",
      "X: 3204+4834$\n",
      "Y: 8038$#\n",
      "O: 7922$^\n",
      "\n",
      "X: 1853-2096$\n",
      "Y: -243$#\n",
      "O: -106$^\n",
      "\n",
      "X: 4679+6367$\n",
      "Y: 11046$\n",
      "O: 11242$\n",
      "\n",
      "0\n",
      "Train: epoch 3\n",
      "Epoch: [3/10], step: [1/625], loss: 1.422339\n",
      "Epoch: [3/10], step: [201/625], loss: 1.455282\n",
      "Epoch: [3/10], step: [401/625], loss: 1.408459\n",
      "Epoch: [3/10], step: [601/625], loss: 1.383311\n",
      "Test: epoch 3 loss: 1.3355594\n",
      "X: 8208-7624$\n",
      "Y: 584$##\n",
      "O: 1022$^\n",
      "\n",
      "X: 3091-3001$\n",
      "Y: 90$###\n",
      "O: -222$^\n",
      "\n",
      "X: 5656+8164$\n",
      "Y: 13820$\n",
      "O: 14113$\n",
      "\n",
      "0\n",
      "Train: epoch 4\n",
      "Epoch: [4/10], step: [1/625], loss: 1.382264\n",
      "Epoch: [4/10], step: [201/625], loss: 1.377869\n",
      "Epoch: [4/10], step: [401/625], loss: 1.424999\n",
      "Epoch: [4/10], step: [601/625], loss: 1.331130\n",
      "Test: epoch 4 loss: 1.2910373\n",
      "X: 2259-3721$\n",
      "Y: -1462$\n",
      "O: -1188$\n",
      "\n",
      "X: 9031+6170$\n",
      "Y: 15201$\n",
      "O: 15110$\n",
      "\n",
      "X: 5780+8377$\n",
      "Y: 14157$\n",
      "O: 14268$\n",
      "\n",
      "0\n",
      "Train: epoch 5\n",
      "Epoch: [5/10], step: [1/625], loss: 1.318288\n",
      "Epoch: [5/10], step: [201/625], loss: 1.336723\n",
      "Epoch: [5/10], step: [401/625], loss: 1.319819\n",
      "Epoch: [5/10], step: [601/625], loss: 1.295830\n",
      "Test: epoch 5 loss: 1.2590843\n",
      "X: 5964-4272$\n",
      "Y: 1692$#\n",
      "O: 1989$^\n",
      "\n",
      "X: 3598+2040$\n",
      "Y: 5638$#\n",
      "O: 5369$^\n",
      "\n",
      "X: 1681+1577$\n",
      "Y: 3258$#\n",
      "O: 3111$^\n",
      "\n",
      "0\n",
      "Train: epoch 6\n",
      "Epoch: [6/10], step: [1/625], loss: 1.261326\n",
      "Epoch: [6/10], step: [201/625], loss: 1.248394\n",
      "Epoch: [6/10], step: [401/625], loss: 1.287027\n",
      "Epoch: [6/10], step: [601/625], loss: 1.234544\n",
      "Test: epoch 6 loss: 1.1791626\n",
      "X: 590+6092$#\n",
      "Y: 6682$#\n",
      "O: 6857$^\n",
      "\n",
      "X: 1255+7861$\n",
      "Y: 9116$#\n",
      "O: 9159$^\n",
      "\n",
      "X: 294+3287$#\n",
      "Y: 3581$#\n",
      "O: 3374$^\n",
      "\n",
      "1\n",
      "Train: epoch 7\n",
      "Epoch: [7/10], step: [1/625], loss: 1.198353\n",
      "Epoch: [7/10], step: [201/625], loss: 1.210413\n",
      "Epoch: [7/10], step: [401/625], loss: 1.168425\n",
      "Epoch: [7/10], step: [601/625], loss: 1.066224\n",
      "Test: epoch 7 loss: 1.0214108\n",
      "X: 7731-3031$\n",
      "Y: 4700$#\n",
      "O: 4701$^\n",
      "\n",
      "X: 489-8306$#\n",
      "Y: -7817$\n",
      "O: -7811$\n",
      "\n",
      "X: 7031+315$#\n",
      "Y: 7346$#\n",
      "O: 7400$^\n",
      "\n",
      "0\n",
      "Train: epoch 8\n",
      "Epoch: [8/10], step: [1/625], loss: 1.112540\n",
      "Epoch: [8/10], step: [201/625], loss: 1.061299\n",
      "Epoch: [8/10], step: [401/625], loss: 1.033911\n",
      "Epoch: [8/10], step: [601/625], loss: 0.982323\n",
      "Test: epoch 8 loss: 0.9526228\n",
      "X: 6382-316$#\n",
      "Y: 6066$#\n",
      "O: 5094$^\n",
      "\n",
      "X: 2464-9490$\n",
      "Y: -7026$\n",
      "O: -7024$\n",
      "\n",
      "X: 6868+5140$\n",
      "Y: 12008$\n",
      "O: 12020$\n",
      "\n",
      "0\n",
      "Train: epoch 9\n",
      "Epoch: [9/10], step: [1/625], loss: 1.004772\n",
      "Epoch: [9/10], step: [201/625], loss: 0.990296\n",
      "Epoch: [9/10], step: [401/625], loss: 0.994292\n",
      "Epoch: [9/10], step: [601/625], loss: 0.982723\n",
      "Test: epoch 9 loss: 0.9118535\n",
      "X: 806-2116$#\n",
      "Y: -1310$\n",
      "O: -1308$\n",
      "\n",
      "X: 9934-7302$\n",
      "Y: 2632$#\n",
      "O: 2619$^\n",
      "\n",
      "X: 9727+2368$\n",
      "Y: 12095$\n",
      "O: 12102$\n",
      "\n",
      "0\n",
      "Train: epoch 10\n",
      "Epoch: [10/10], step: [1/625], loss: 0.954426\n",
      "Epoch: [10/10], step: [201/625], loss: 0.949677\n",
      "Epoch: [10/10], step: [401/625], loss: 0.977489\n",
      "Epoch: [10/10], step: [601/625], loss: 0.949732\n",
      "Test: epoch 10 loss: 0.8684076\n",
      "X: 3712+6419$\n",
      "Y: 10131$\n",
      "O: 10144$\n",
      "\n",
      "X: 5840+1313$\n",
      "Y: 7153$#\n",
      "O: 7103$^\n",
      "\n",
      "X: 5648+6570$\n",
      "Y: 12218$\n",
      "O: 12244$\n",
      "\n",
      "0\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "            \n",
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):  \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        # prepare the data (X_batch and Y_batch) for training\n",
    "        X_ids, X_sent_lens = batch_to_ids(X_batch, word2id, max_len=max_len)\n",
    "        Y_ids, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len=max_len)\n",
    "        # using function batch_to_ids\n",
    "        predictions, loss = model.train_on_batch(\n",
    "            session,\n",
    "            X_ids,\n",
    "            X_sent_lens,\n",
    "            Y_ids,\n",
    "            Y_sent_lens,\n",
    "            learning_rate,\n",
    "            dropout_keep_probability\n",
    "        )\n",
    "        \n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "                \n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "    # prepare test data (X_sent and Y_sent) for predicting \n",
    "    X, X_sent_lens = batch_to_ids(X_sent, word2id, max_len=max_len)\n",
    "    Y, Y_sent_lens = batch_to_ids(Y_sent, word2id, max_len=max_len)\n",
    "    # quality and computing value of the loss function\n",
    "    # using function batch_to_ids\n",
    "    \n",
    "    predictions, loss = model.predict_for_batch_with_loss(\n",
    "        session,\n",
    "        X,\n",
    "        X_sent_lens,\n",
    "        Y,\n",
    "        Y_sent_lens\n",
    "    )\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss,)\n",
    "    for x, y, p  in list(zip(X, Y, predictions))[:3]:\n",
    "        print('X:',''.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:',''.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:',''.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "\n",
    "    model_predictions = []\n",
    "    ground_truth = []\n",
    "    invalid_number_prediction_count = 0\n",
    "    # For the whole test set calculate ground-truth values (as integer numbers) and prediction values (also as integers) to calculate metrics.\n",
    "    for X_batch, Y_batch in generate_batches(test_set, batch_size=batch_size):\n",
    "        X_ids, X_sent_lens = batch_to_ids(X_batch, word2id, max_len=max_len)\n",
    "        Y_ids, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len=max_len)\n",
    "        predictions = model.predict_for_batch(session, X_ids, X_sent_lens)\n",
    "        for y, p in zip(Y_ids, predictions):\n",
    "            y_sent = ''.join(ids_to_sentence(y, id2word))\n",
    "            y_sent = y_sent[:y_sent.find('$')]\n",
    "            p_sent = ''.join(ids_to_sentence(p, id2word))\n",
    "            p_sent = p_sent[:p_sent.find('$')]\n",
    "            if p_sent.isdigit() or (p_sent.startswith('-') and p_sent[1:].isdigit()):\n",
    "                model_predictions.append(int(p_sent))\n",
    "                ground_truth.append(int(y_sent))\n",
    "            else:\n",
    "                invalid_number_prediction_count += 1\n",
    "    all_model_predictions.append(model_predictions)\n",
    "    all_ground_truth.append(ground_truth)\n",
    "    invalid_number_prediction_counts.append(invalid_number_prediction_count)\n",
    "    print(invalid_number_prediction_count)\n",
    "            \n",
    "print('\\n...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, MAE: 949.963400, Invalid numbers: 0\n",
      "Epoch: 2, MAE: 337.342200, Invalid numbers: 0\n",
      "Epoch: 3, MAE: 234.103500, Invalid numbers: 0\n",
      "Epoch: 4, MAE: 195.828450, Invalid numbers: 0\n",
      "Epoch: 5, MAE: 171.797500, Invalid numbers: 0\n",
      "Epoch: 6, MAE: 104.704935, Invalid numbers: 1\n",
      "Epoch: 7, MAE: 54.836600, Invalid numbers: 0\n",
      "Epoch: 8, MAE: 54.332850, Invalid numbers: 0\n",
      "Epoch: 9, MAE: 37.148500, Invalid numbers: 0\n",
      "Epoch: 10, MAE: 27.736800, Invalid numbers: 0\n"
     ]
    }
   ],
   "source": [
    "# Use MAE metric to evaluate the trained model\n",
    "for i, (gts, predictions, invalid_number_prediction_count) in enumerate(zip(all_ground_truth,\n",
    "                                                                            all_model_predictions,\n",
    "                                                                            invalid_number_prediction_counts), 1):\n",
    "    mae = mean_absolute_error(gts, predictions)\n",
    "    print(\"Epoch: %i, MAE: %f, Invalid numbers: %i\" % (i, mae, invalid_number_prediction_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
